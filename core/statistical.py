"""
í†µê³„ ë¶„ì„ ëª¨ë“ˆ (v2.0 ì‹ ê·œ)
ë‹¤ì¤‘ íŒŒì¼ í†µê³„ ë¹„êµ ë° ì´ìƒì¹˜ ìë™ íƒì§€ ë„êµ¬ì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- HSV/LAB ìƒ‰ê³µê°„ ë¶„í¬ ë¶„ì„
- í†µê³„ì  ì´ìƒì¹˜ ìë™ íƒì§€
- RGB-to-ASCII íŒ¨í„´ ì¸ì‹  
- ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ë¶„ì„
- CTF ë¬¸ì œ: Turtles All The Way Down í•´ê²°
"""

import os
import numpy as np
from PIL import Image
import colorsys
from typing import Dict, List, Optional, Any, Tuple
import statistics
from collections import Counter
import base64
import hashlib
import re

class StatisticalAnalyzer:
    """í†µê³„ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.supported_formats = ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']
    
    def analyze_multiple_files(self, file_list: List[str]) -> Dict[str, Any]:
        """ë‹¤ì¤‘ íŒŒì¼ í†µê³„ ë¹„êµ ë¶„ì„"""
        print(f"ğŸ” {len(file_list)}ê°œ íŒŒì¼ í†µê³„ ë¶„ì„ ì‹œì‘...")
        
        results = {
            'file_count': len(file_list),
            'file_analyses': {},
            'statistical_summary': {},
            'outliers': [],
            'patterns': {},
            'recommendations': []
        }
        
        # ê°œë³„ íŒŒì¼ ë¶„ì„
        analyses = {}
        for i, file_path in enumerate(file_list, 1):
            print(f"  ğŸ“Š {i}/{len(file_list)}: {os.path.basename(file_path)}")
            try:
                analysis = self.analyze_single_file(file_path)
                analyses[file_path] = analysis
            except Exception as e:
                print(f"    âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
                analyses[file_path] = {'error': str(e)}
        
        results['file_analyses'] = analyses
        
        # í†µê³„ì  ë¹„êµ ë¶„ì„
        print("\nğŸ“ˆ í†µê³„ì  ë¹„êµ ë¶„ì„ ì¤‘...")
        results['statistical_summary'] = self._compare_statistics(analyses)
        
        # ì´ìƒì¹˜ íƒì§€
        print("ğŸ¯ ì´ìƒì¹˜ ìë™ íƒì§€ ì¤‘...")
        results['outliers'] = self._detect_outliers(analyses)
        
        # íŒ¨í„´ ì¸ì‹
        print("ğŸ” ìˆ¨ê²¨ì§„ íŒ¨í„´ ê²€ìƒ‰ ì¤‘...")
        results['patterns'] = self._detect_patterns(analyses)
        
        # ì¶”ì²œ ì‚¬í•­
        results['recommendations'] = self._generate_recommendations(results)
        
        return results
    
    def analyze_single_file(self, file_path: str) -> Dict[str, Any]:
        """ê°œë³„ íŒŒì¼ ìƒì„¸ ë¶„ì„"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        img = Image.open(file_path)
        img_array = np.array(img)
        
        analysis = {
            'file_path': file_path,
            'file_size': os.path.getsize(file_path),
            'dimensions': img.size,
            'mode': img.mode,
            'channels': len(img.getbands()),
            'aspect_ratio': img.size[0] / img.size[1] if img.size[1] > 0 else 0,
            'is_square': img.size[0] == img.size[1],
        }
        
        # RGB ë¶„ì„
        if len(img_array.shape) >= 3:
            analysis.update(self._analyze_rgb_distribution(img_array))
            analysis.update(self._analyze_hsv_distribution(img_array))
            analysis.update(self._calculate_entropy(img_array))
            analysis.update(self._detect_ascii_patterns(img_array))
        
        # íŒŒì¼ í•´ì‹œ
        analysis['file_hash'] = self._calculate_file_hash(file_path)
        
        return analysis
    
    def _analyze_rgb_distribution(self, img_array: np.ndarray) -> Dict[str, Any]:
        """RGB ìƒ‰ìƒ ë¶„í¬ ë¶„ì„"""
        rgb_stats = {}
        
        for i, channel in enumerate(['red', 'green', 'blue']):
            if i < img_array.shape[2]:
                channel_data = img_array[:, :, i].flatten()
                rgb_stats[f'{channel}_mean'] = float(np.mean(channel_data))
                rgb_stats[f'{channel}_std'] = float(np.std(channel_data))
                rgb_stats[f'{channel}_min'] = int(np.min(channel_data))
                rgb_stats[f'{channel}_max'] = int(np.max(channel_data))
                
                # íˆìŠ¤í† ê·¸ë¨ ë¶„ì„
                hist, _ = np.histogram(channel_data, bins=256, range=(0, 256))
                rgb_stats[f'{channel}_histogram_peaks'] = len([i for i, v in enumerate(hist) if v > np.mean(hist) * 2])
                rgb_stats[f'{channel}_dominant_values'] = [int(i) for i in np.argsort(hist)[-5:]]
        
        return {'rgb_analysis': rgb_stats}
    
    def _analyze_hsv_distribution(self, img_array: np.ndarray) -> Dict[str, Any]:
        """HSV ìƒ‰ê³µê°„ ë¶„í¬ ë¶„ì„ - CTF í•µì‹¬!"""
        # RGB â†’ HSV ë³€í™˜ (4ì±„ë„ RGBA ì²˜ë¦¬)
        if len(img_array.shape) >= 3 and img_array.shape[2] == 4:  # RGBA
            rgb_array = img_array[:, :, :3]  # ì•ŒíŒŒ ì±„ë„ ì œê±°
        elif len(img_array.shape) >= 3:
            rgb_array = img_array
        else:
            return {'hsv_analysis': {'error': 'Invalid image format'}}
            
        hsv_array = np.zeros((rgb_array.shape[0], rgb_array.shape[1], 3), dtype=np.float32)
        
        for i in range(rgb_array.shape[0]):
            for j in range(rgb_array.shape[1]):
                r, g, b = rgb_array[i, j, :3] / 255.0
                h, s, v = colorsys.rgb_to_hsv(r, g, b)
                hsv_array[i, j] = [h * 360, s * 100, v * 100]  # ì •ê·œí™”
        
        hsv_stats = {}
        for i, channel in enumerate(['hue', 'saturation', 'value']):
            channel_data = hsv_array[:, :, i].flatten()
            hsv_stats[f'{channel}_mean'] = float(np.mean(channel_data))
            hsv_stats[f'{channel}_std'] = float(np.std(channel_data))
            hsv_stats[f'{channel}_variance'] = float(np.var(channel_data))
            
            # ë¶„ì‚°ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ê±°ë‚˜ ë‚®ì€ ê²½ìš° ê°ì§€
            if channel == 'hue':
                hsv_stats['hue_anomaly'] = hsv_stats['hue_variance'] > 10000 or hsv_stats['hue_variance'] < 100
            elif channel == 'saturation':
                hsv_stats['saturation_anomaly'] = hsv_stats['saturation_variance'] > 1000 or hsv_stats['saturation_variance'] < 10
            elif channel == 'value':
                hsv_stats['value_anomaly'] = hsv_stats['value_variance'] > 1000 or hsv_stats['value_variance'] < 50
        
        return {'hsv_analysis': hsv_stats}
    
    def _calculate_entropy(self, img_array: np.ndarray) -> Dict[str, Any]:
        """ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ë¬´ì‘ìœ„ì„± ë¶„ì„"""
        entropy_stats = {}
        
        # ì „ì²´ ì´ë¯¸ì§€ ì—”íŠ¸ë¡œí”¼
        flattened = img_array.flatten()
        hist, _ = np.histogram(flattened, bins=256, range=(0, 256))
        hist = hist / hist.sum()  # ì •ê·œí™”
        entropy = -np.sum(hist * np.log2(hist + 1e-10))  # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        
        entropy_stats['total_entropy'] = float(entropy)
        entropy_stats['entropy_normalized'] = float(entropy / 8.0)  # 8ë¹„íŠ¸ ê¸°ì¤€ ì •ê·œí™”
        
        # ì±„ë„ë³„ ì—”íŠ¸ë¡œí”¼
        if len(img_array.shape) >= 3:
            for i, channel in enumerate(['red', 'green', 'blue']):
                if i < img_array.shape[2]:
                    channel_data = img_array[:, :, i].flatten()
                    hist, _ = np.histogram(channel_data, bins=256, range=(0, 256))
                    hist = hist / hist.sum()
                    channel_entropy = -np.sum(hist * np.log2(hist + 1e-10))
                    entropy_stats[f'{channel}_entropy'] = float(channel_entropy)
        
        return {'entropy_analysis': entropy_stats}
    
    def _detect_ascii_patterns(self, img_array: np.ndarray) -> Dict[str, Any]:
        """RGB-to-ASCII íŒ¨í„´ ìë™ ê°ì§€"""
        patterns = {
            'ascii_candidates': [],
            'text_probability': 0.0,
            'readable_sequences': []
        }
        
        # RGB ê°’ë“¤ì„ ASCIIë¡œ í•´ì„ ì‹œë„
        if len(img_array.shape) >= 3 and img_array.shape[2] >= 3:
            # ì´ë¯¸ì§€ë¥¼ 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜
            rgb_flat = img_array.reshape(-1, img_array.shape[2])
            
            ascii_candidates = []
            for pixel in rgb_flat[:1000]:  # ì²˜ìŒ 1000í”½ì…€ë§Œ ê²€ì‚¬ (ì„±ëŠ¥)
                for value in pixel[:3]:  # RGB ì±„ë„
                    if 32 <= value <= 126:  # ì¸ì‡„ ê°€ëŠ¥í•œ ASCII ë²”ìœ„
                        ascii_candidates.append(chr(value))
            
            patterns['ascii_candidates'] = ascii_candidates[:100]  # ì²˜ìŒ 100ê°œë§Œ
            
            # í…ìŠ¤íŠ¸ íŒ¨í„´ í™•ë¥  ê³„ì‚°
            printable_ratio = len([c for c in ascii_candidates if c.isprintable()]) / max(len(ascii_candidates), 1)
            patterns['text_probability'] = printable_ratio
            
            # ì—°ì†ëœ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ìì—´ ì°¾ê¸°
            ascii_string = ''.join(ascii_candidates)
            readable_sequences = []
            current_seq = ""
            
            for char in ascii_string:
                if char.isalnum() or char in ' .,!?-_':
                    current_seq += char
                else:
                    if len(current_seq) >= 4:  # 4ê¸€ì ì´ìƒì˜ ì‹œí€€ìŠ¤ë§Œ
                        readable_sequences.append(current_seq.strip())
                    current_seq = ""
            
            # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ì²˜ë¦¬
            if len(current_seq) >= 4:
                readable_sequences.append(current_seq.strip())
            
            patterns['readable_sequences'] = readable_sequences[:10]  # ìƒìœ„ 10ê°œë§Œ
        
        return {'pattern_analysis': patterns}
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()
    
    def _compare_statistics(self, analyses: Dict[str, Dict]) -> Dict[str, Any]:
        """í†µê³„ì  ë¹„êµ ë¶„ì„"""
        summary = {
            'dimension_stats': {},
            'color_stats': {},
            'entropy_stats': {},
            'file_size_stats': {}
        }
        
        valid_analyses = {k: v for k, v in analyses.items() if 'error' not in v}
        
        if not valid_analyses:
            return summary
        
        # í¬ê¸° í†µê³„
        widths = [analysis['dimensions'][0] for analysis in valid_analyses.values()]
        heights = [analysis['dimensions'][1] for analysis in valid_analyses.values()]
        aspect_ratios = [analysis['aspect_ratio'] for analysis in valid_analyses.values()]
        file_sizes = [analysis['file_size'] for analysis in valid_analyses.values()]
        
        summary['dimension_stats'] = {
            'width_mean': statistics.mean(widths),
            'width_std': statistics.stdev(widths) if len(widths) > 1 else 0,
            'height_mean': statistics.mean(heights),
            'height_std': statistics.stdev(heights) if len(heights) > 1 else 0,
            'aspect_ratio_mean': statistics.mean(aspect_ratios),
            'aspect_ratio_std': statistics.stdev(aspect_ratios) if len(aspect_ratios) > 1 else 0,
            'square_count': sum(1 for analysis in valid_analyses.values() if analysis['is_square'])
        }
        
        summary['file_size_stats'] = {
            'size_mean': statistics.mean(file_sizes),
            'size_std': statistics.stdev(file_sizes) if len(file_sizes) > 1 else 0,
            'size_min': min(file_sizes),
            'size_max': max(file_sizes)
        }
        
        # HSV ë¶„ì‚° í†µê³„ (í•µì‹¬!)
        hsv_variances = []
        for analysis in valid_analyses.values():
            if 'hsv_analysis' in analysis:
                hsv_var = (
                    analysis['hsv_analysis']['hue_variance'] +
                    analysis['hsv_analysis']['saturation_variance'] + 
                    analysis['hsv_analysis']['value_variance']
                ) / 3
                hsv_variances.append(hsv_var)
        
        if hsv_variances:
            summary['color_stats'] = {
                'hsv_variance_mean': statistics.mean(hsv_variances),
                'hsv_variance_std': statistics.stdev(hsv_variances) if len(hsv_variances) > 1 else 0
            }
        
        return summary
    
    def _detect_outliers(self, analyses: Dict[str, Dict]) -> List[Dict[str, Any]]:
        """ì´ìƒì¹˜ ìë™ íƒì§€"""
        outliers = []
        valid_analyses = {k: v for k, v in analyses.items() if 'error' not in v}
        
        if len(valid_analyses) < 3:
            return outliers
        
        # íŒŒì¼ í¬ê¸° ì´ìƒì¹˜
        file_sizes = [(path, analysis['file_size']) for path, analysis in valid_analyses.items()]
        size_mean = statistics.mean([size for _, size in file_sizes])
        size_std = statistics.stdev([size for _, size in file_sizes]) if len(file_sizes) > 1 else 0
        
        for path, size in file_sizes:
            if size_std > 0:
                z_score = abs((size - size_mean) / size_std)
                if z_score > 2.0:  # 2 í‘œì¤€í¸ì°¨ ì´ìƒ
                    outliers.append({
                        'file_path': path,
                        'type': 'file_size_outlier',
                        'z_score': z_score,
                        'value': size,
                        'mean': size_mean,
                        'severity': 'high' if z_score > 3.0 else 'medium'
                    })
        
        # ì •ì‚¬ê°í˜• í˜•íƒœ ì´ìƒì¹˜ (íŒíŠ¸ ê¸°ë°˜)
        square_files = [path for path, analysis in valid_analyses.items() if analysis['is_square']]
        non_square_files = [path for path, analysis in valid_analyses.items() if not analysis['is_square']]
        
        # ëŒ€ë¶€ë¶„ì´ ì •ì‚¬ê°í˜•ì´ ì•„ë‹Œë° ëª‡ ê°œë§Œ ì •ì‚¬ê°í˜•ì´ë©´ ì´ìƒì¹˜
        if len(square_files) < len(non_square_files) * 0.3 and len(square_files) > 0:
            for path in square_files:
                outliers.append({
                    'file_path': path,
                    'type': 'dimension_outlier',
                    'reason': 'square_among_non_square',
                    'severity': 'high',
                    'hint_match': True  # íŒíŠ¸ì™€ ë§¤ì¹˜ë¨
                })
        
        # HSV ë¶„ì‚° ì´ìƒì¹˜ (ìƒ‰ìƒ ë¶„í¬ ì´ìƒ)
        hsv_data = []
        for path, analysis in valid_analyses.items():
            if 'hsv_analysis' in analysis:
                total_variance = (
                    analysis['hsv_analysis']['hue_variance'] +
                    analysis['hsv_analysis']['saturation_variance'] + 
                    analysis['hsv_analysis']['value_variance']
                )
                hsv_data.append((path, total_variance))
        
        if len(hsv_data) > 2:
            hsv_mean = statistics.mean([var for _, var in hsv_data])
            hsv_std = statistics.stdev([var for _, var in hsv_data]) if len(hsv_data) > 1 else 0
            
            for path, variance in hsv_data:
                if hsv_std > 0:
                    z_score = abs((variance - hsv_mean) / hsv_std)
                    if z_score > 1.5:  # HSVëŠ” ë” ë¯¼ê°í•˜ê²Œ íƒì§€
                        outliers.append({
                            'file_path': path,
                            'type': 'color_distribution_outlier',
                            'z_score': z_score,
                            'variance': variance,
                            'severity': 'high' if z_score > 2.5 else 'medium'
                        })
        
        # ì—”íŠ¸ë¡œí”¼ ì´ìƒì¹˜
        entropy_data = []
        for path, analysis in valid_analyses.items():
            if 'entropy_analysis' in analysis:
                entropy_data.append((path, analysis['entropy_analysis']['total_entropy']))
        
        if len(entropy_data) > 2:
            entropy_mean = statistics.mean([ent for _, ent in entropy_data])
            entropy_std = statistics.stdev([ent for _, ent in entropy_data]) if len(entropy_data) > 1 else 0
            
            for path, entropy in entropy_data:
                if entropy_std > 0:
                    z_score = abs((entropy - entropy_mean) / entropy_std)
                    if z_score > 2.0:
                        outliers.append({
                            'file_path': path,
                            'type': 'entropy_outlier',
                            'z_score': z_score,
                            'entropy': entropy,
                            'severity': 'medium'
                        })
        
        return outliers
    
    def _detect_patterns(self, analyses: Dict[str, Dict]) -> Dict[str, Any]:
        """ìˆ¨ê²¨ì§„ íŒ¨í„´ ê²€ìƒ‰"""
        patterns = {
            'ascii_patterns': [],
            'hash_patterns': [],
            'dimension_patterns': [],
            'suspicious_files': []
        }
        
        valid_analyses = {k: v for k, v in analyses.items() if 'error' not in v}
        
        # ASCII íŒ¨í„´ ê²€ìƒ‰
        for path, analysis in valid_analyses.items():
            if 'pattern_analysis' in analysis:
                pattern_data = analysis['pattern_analysis']
                if pattern_data['text_probability'] > 0.1:  # 10% ì´ìƒ í…ìŠ¤íŠ¸ í™•ë¥ 
                    patterns['ascii_patterns'].append({
                        'file_path': path,
                        'text_probability': pattern_data['text_probability'],
                        'readable_sequences': pattern_data['readable_sequences'],
                        'ascii_sample': pattern_data['ascii_candidates'][:50]
                    })
        
        # ì°¨ì› íŒ¨í„´
        dimensions = [(path, analysis['dimensions']) for path, analysis in valid_analyses.items()]
        dimension_counter = Counter([dim for _, dim in dimensions])
        
        for (width, height), count in dimension_counter.most_common():
            if count == 1 and len(dimensions) > 5:  # ìœ ì¼í•œ í¬ê¸°
                matching_files = [path for path, dim in dimensions if dim == (width, height)]
                patterns['dimension_patterns'].append({
                    'dimension': (width, height),
                    'files': matching_files,
                    'uniqueness': 'unique_size',
                    'suspicion_level': 'high'
                })
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒŒì¼ ì¢…í•©
        suspicious_indicators = {}
        
        for path, analysis in valid_analyses.items():
            suspicion_score = 0
            reasons = []
            
            # ì •ì‚¬ê°í˜•ì´ë©´ ì˜ì‹¬ë„ +3
            if analysis['is_square']:
                suspicion_score += 3
                reasons.append('square_dimension')
            
            # í…ìŠ¤íŠ¸ íŒ¨í„´ì´ ìˆìœ¼ë©´ ì˜ì‹¬ë„ +2
            if 'pattern_analysis' in analysis and analysis['pattern_analysis']['text_probability'] > 0.1:
                suspicion_score += 2
                reasons.append('ascii_pattern_detected')
            
            # HSV ë¶„ì‚° ì´ìƒì¹˜ë©´ ì˜ì‹¬ë„ +2
            if 'hsv_analysis' in analysis:
                hsv = analysis['hsv_analysis']
                if hsv.get('hue_anomaly') or hsv.get('saturation_anomaly') or hsv.get('value_anomaly'):
                    suspicion_score += 2
                    reasons.append('color_anomaly')
            
            if suspicion_score >= 3:  # ì˜ì‹¬ë„ 3 ì´ìƒ
                patterns['suspicious_files'].append({
                    'file_path': path,
                    'suspicion_score': suspicion_score,
                    'reasons': reasons,
                    'priority': 'high' if suspicion_score >= 5 else 'medium'
                })
        
        return patterns
    
    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì´ìƒì¹˜ê°€ ë°œê²¬ëœ ê²½ìš°
        if results['outliers']:
            high_priority_outliers = [o for o in results['outliers'] if o.get('severity') == 'high']
            if high_priority_outliers:
                recommendations.append("ğŸ¯ ê³ ìš°ì„ ìˆœìœ„ ì´ìƒì¹˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. í•´ë‹¹ íŒŒì¼ë“¤ì„ ì§‘ì¤‘ ë¶„ì„í•˜ì„¸ìš”.")
                for outlier in high_priority_outliers[:3]:  # ìƒìœ„ 3ê°œë§Œ
                    filename = os.path.basename(outlier['file_path'])
                    recommendations.append(f"   â†’ {filename}: {outlier['type']}")
        
        # íŒíŠ¸ì™€ ë§¤ì¹­ë˜ëŠ” íŒ¨í„´
        hint_matches = [o for o in results['outliers'] if o.get('hint_match')]
        if hint_matches:
            recommendations.append("ğŸ’¡ CTF íŒíŠ¸ì™€ ì¼ì¹˜í•˜ëŠ” íŒ¨í„´ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ASCII íŒ¨í„´ ë°œê²¬
        if results['patterns']['ascii_patterns']:
            recommendations.append("ğŸ“ ASCII í…ìŠ¤íŠ¸ íŒ¨í„´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. RGB ê°’ì„ ë¬¸ìë¡œ ë³€í™˜í•´ë³´ì„¸ìš”.")
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒŒì¼
        suspicious = results['patterns']['suspicious_files']
        high_suspicious = [s for s in suspicious if s['priority'] == 'high']
        if high_suspicious:
            recommendations.append(f"ğŸš¨ {len(high_suspicious)}ê°œì˜ ë§¤ìš° ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒŒì¼ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            for sus in high_suspicious[:2]:  # ìƒìœ„ 2ê°œë§Œ
                filename = os.path.basename(sus['file_path'])
                recommendations.append(f"   â†’ {filename} (ì ìˆ˜: {sus['suspicion_score']})")
        
        if not recommendations:
            recommendations.append("âœ… íŠ¹ë³„í•œ ì´ìƒì¹˜ëŠ” ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë¶„ì„ ë°©ë²•ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
        
        return recommendations
    
    def search_for_flags(self, analysis_result: Dict[str, Any]) -> List[str]:
        """ë¶„ì„ ê²°ê³¼ì—ì„œ CTF í”Œë˜ê·¸ íŒ¨í„´ ê²€ìƒ‰"""
        flags = []
        flag_patterns = [
            r'flag\{[^}]+\}',
            r'FLAG\{[^}]+\}', 
            r'CTF\{[^}]+\}',
            r'ctf\{[^}]+\}'
        ]
        
        # ASCII íŒ¨í„´ì—ì„œ í”Œë˜ê·¸ ê²€ìƒ‰
        if 'patterns' in analysis_result and 'ascii_patterns' in analysis_result['patterns']:
            for pattern_data in analysis_result['patterns']['ascii_patterns']:
                # readable_sequencesì—ì„œ ê²€ìƒ‰
                for sequence in pattern_data.get('readable_sequences', []):
                    for flag_pattern in flag_patterns:
                        matches = re.findall(flag_pattern, sequence, re.IGNORECASE)
                        flags.extend(matches)
                
                # ascii_sampleì—ì„œë„ ê²€ìƒ‰
                ascii_string = ''.join(pattern_data.get('ascii_sample', []))
                for flag_pattern in flag_patterns:
                    matches = re.findall(flag_pattern, ascii_string, re.IGNORECASE)
                    flags.extend(matches)
        
        return list(set(flags))  # ì¤‘ë³µ ì œê±°


def main():
    """í†µê³„ ë¶„ì„ ë„êµ¬ í…ŒìŠ¤íŠ¸"""
    analyzer = StatisticalAnalyzer()
    
    # Turtles All The Way Down CTF ë¬¸ì œ í…ŒìŠ¤íŠ¸
    test_dir = "CTF-ë¬¸ì œ-ì‚¬ì§„/turtles_all_the_way_down/ctf"
    
    if os.path.exists(test_dir):
        print("ğŸ¢ Turtles All The Way Down CTF ë¬¸ì œ ë¶„ì„ ì‹œì‘")
        print("=" * 60)
        
        # ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘
        file_list = []
        for filename in os.listdir(test_dir):
            if any(filename.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.bmp']):
                file_list.append(os.path.join(test_dir, filename))
        
        file_list.sort()  # ì •ë ¬
        print(f"ğŸ“ ì´ {len(file_list)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # í†µê³„ ë¶„ì„ ì‹¤í–‰
        result = analyzer.analyze_multiple_files(file_list)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ë¶„ì„ ì™„ë£Œ: {result['file_count']}ê°œ íŒŒì¼")
        
        # ì´ìƒì¹˜ ì¶œë ¥
        if result['outliers']:
            print(f"\nğŸ¯ ì´ìƒì¹˜ {len(result['outliers'])}ê°œ ë°œê²¬:")
            for outlier in result['outliers']:
                filename = os.path.basename(outlier['file_path'])
                print(f"  ğŸš¨ {filename}")
                print(f"     íƒ€ì…: {outlier['type']}")
                print(f"     ì‹¬ê°ë„: {outlier['severity']}")
                if 'z_score' in outlier:
                    print(f"     Z-score: {outlier['z_score']:.2f}")
                if outlier.get('hint_match'):
                    print(f"     ğŸ’¡ íŒíŠ¸ ë§¤ì¹˜!")
                print()
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒŒì¼
        suspicious = result['patterns']['suspicious_files']
        if suspicious:
            print(f"ğŸ” ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒŒì¼ {len(suspicious)}ê°œ:")
            for sus in suspicious:
                filename = os.path.basename(sus['file_path'])
                print(f"  ğŸ¯ {filename} (ì ìˆ˜: {sus['suspicion_score']})")
                print(f"     ì´ìœ : {', '.join(sus['reasons'])}")
                print()
        
        # í”Œë˜ê·¸ ê²€ìƒ‰
        flags = analyzer.search_for_flags(result)
        if flags:
            print("ğŸš© ë°œê²¬ëœ í”Œë˜ê·¸:")
            for flag in flags:
                print(f"  âœ… {flag}")
        else:
            print("âŒ í”Œë˜ê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ì¶”ì²œì‚¬í•­
        if result['recommendations']:
            print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
            for rec in result['recommendations']:
                print(f"  {rec}")
        
        print("\n" + "=" * 60)
    else:
        print("âŒ í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")


if __name__ == "__main__":
    main()